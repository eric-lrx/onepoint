#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
compare_prompts.py - Outil de comparaison directe de prompts pour Ollama
"""

import sys
import time
import json
import argparse
import statistics
import subprocess
import threading
import queue
import re
import os
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple
import uuid
import tempfile
import seaborn as sns

# Configuration
DEFAULT_MODEL = "deepseek-r1:7b"
DEFAULT_REPETITIONS = 1
RESOURCE_SAMPLING_RATE = 0.5  # en secondes

@dataclass
class PromptResult:
    """R√©sultat de l'ex√©cution d'un prompt"""
    prompt_id: str
    prompt_text: str
    prompt_length: int
    response_text: str
    response_length: int
    tokens_in: int = 0
    tokens_out: int = 0
    execution_time: float = 0.0
    avg_cpu_usage: float = 0.0
    max_cpu_usage: float = 0.0
    avg_memory_mb: float = 0.0
    max_memory_mb: float = 0.0
    avg_gpu_usage: float = 0.0
    max_gpu_usage: float = 0.0
    energy_estimate_joules: float = 0.0
    eco_score: float = 0.0

class ResourceMonitor:
    """Moniteur de ressources syst√®me en temps r√©el"""
    
    def __init__(self):
        self.cpu_samples = []
        self.memory_samples = []
        self.gpu_samples = []
        self.running = False
        self.queue = queue.Queue()
        
    def start(self):
        """D√©marre le monitoring des ressources"""
        self.cpu_samples = []
        self.memory_samples = []
        self.gpu_samples = []
        self.running = True
        self.thread = threading.Thread(target=self._monitor_loop)
        self.thread.daemon = True
        self.thread.start()
        
    def stop(self):
        """Arr√™te le monitoring des ressources"""
        self.running = False
        if hasattr(self, 'thread'):
            self.thread.join(timeout=1.0)
        
    def _monitor_loop(self):
        """Boucle de monitoring ex√©cut√©e dans un thread s√©par√©"""
        while self.running:
            # Mesurer CPU
            try:
                cpu_percent = self._get_cpu_usage()
                self.cpu_samples.append(cpu_percent)
            except Exception as e:
                print(f"Erreur lors de la mesure CPU: {e}")
            
            # Mesurer m√©moire
            try:
                memory_usage = self._get_memory_usage()
                self.memory_samples.append(memory_usage)
            except Exception as e:
                print(f"Erreur lors de la mesure m√©moire: {e}")
            
            # Mesurer GPU si disponible
            try:
                gpu_usage = self._get_gpu_usage()
                if gpu_usage is not None:
                    self.gpu_samples.append(gpu_usage)
            except Exception:
                pass  # Ignorer les erreurs GPU (peut ne pas √™tre disponible)
            
            time.sleep(RESOURCE_SAMPLING_RATE)
    
    def _get_cpu_usage(self) -> float:
        """Obtenir l'utilisation CPU actuelle en pourcentage"""
        if sys.platform == "linux" or sys.platform == "linux2":
            # Sur Linux, on utilise /proc/stat
            try:
                with open('/proc/stat', 'r') as f:
                    cpu_stats = f.readline().split()
                    user = float(cpu_stats[1])
                    nice = float(cpu_stats[2])
                    system = float(cpu_stats[3])
                    idle = float(cpu_stats[4])
                    total = user + nice + system + idle
                    usage = 100 * (1 - idle / total)
                    return usage
            except:
                # Fallback sur psutil si disponible
                try:
                    import psutil
                    return psutil.cpu_percent(interval=0.1)
                except:
                    # Fallback sur top
                    command = "top -bn1 | grep 'Cpu(s)' | awk '{print $2 + $4}'"
                    output = subprocess.check_output(command, shell=True).decode('utf-8').strip()
                    return float(output)
        else:
            # Sur d'autres plateformes, utiliser psutil si disponible
            try:
                import psutil
                return psutil.cpu_percent(interval=0.1)
            except:
                return 0.0  # Valeur par d√©faut si impossible √† mesurer
            
    def _get_gpu_usage(self) -> Optional[float]:
        """Obtenir l'utilisation GPU actuelle en pourcentage (si disponible)"""
        try:
            # V√©rifier d'abord si nvidia-smi est disponible
            which_output = subprocess.run(["which", "nvidia-smi"], 
                                       stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            if which_output.returncode != 0:
                return None  # nvidia-smi n'est pas install√©
                
            # Si disponible, ex√©cuter la commande
            command = "nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits"
            output = subprocess.check_output(command, shell=True, stderr=subprocess.DEVNULL).decode('utf-8').strip()
            return float(output)
        except (subprocess.CalledProcessError, ValueError, FileNotFoundError):
            # GPU non disponible ou erreur
            return None
    
    def _get_memory_usage(self) -> float:
        """Obtenir l'utilisation m√©moire actuelle en MB"""
        if sys.platform == "linux" or sys.platform == "linux2":
            with open('/proc/meminfo', 'r') as f:
                lines = f.readlines()
                mem_total = float(lines[0].split()[1]) / 1024  # en MB
                mem_free = float(lines[1].split()[1]) / 1024   # en MB
                mem_avail = float(lines[2].split()[1]) / 1024  # en MB
                return mem_total - mem_avail
        else:
            command = "free -m | awk 'NR==2{print $3}'"
            output = subprocess.check_output(command, shell=True).decode('utf-8').strip()
            return float(output)
            
    def get_summary(self) -> Dict[str, float]:
        """Obtenir un r√©sum√© des mesures de ressources"""
        if not self.cpu_samples:
            return {
                "avg_cpu": 0.0,
                "max_cpu": 0.0,
                "avg_memory": 0.0,
                "max_memory": 0.0,
                "avg_gpu": 0.0,
                "max_gpu": 0.0
            }
            
        result = {
            "avg_cpu": statistics.mean(self.cpu_samples),
            "max_cpu": max(self.cpu_samples),
            "avg_memory": statistics.mean(self.memory_samples),
            "max_memory": max(self.memory_samples)
        }
        
        if self.gpu_samples:
            result["avg_gpu"] = statistics.mean(self.gpu_samples)
            result["max_gpu"] = max(self.gpu_samples)
        else:
            result["avg_gpu"] = 0.0
            result["max_gpu"] = 0.0
            
        return result

class PromptRunner:
    """Ex√©cuteur de prompts pour Ollama"""
    
    def __init__(self, model: str = DEFAULT_MODEL):
        self.model = model
        self.resource_monitor = ResourceMonitor()
        
    def run_prompt(self, prompt_id: str, prompt_text: str) -> PromptResult:
        """Ex√©cute un prompt et renvoie les mesures de performance"""
        print(f"Ex√©cution du prompt: {prompt_id}")
        print(f"  Texte: {prompt_text[:50]}...")
        
        # Initialiser le r√©sultat
        result = PromptResult(
            prompt_id=prompt_id,
            prompt_text=prompt_text,
            prompt_length=len(prompt_text),
            response_text="",
            response_length=0
        )
        
        # V√©rifier si Ollama est disponible
        try:
            subprocess.run(["ollama", "list"], capture_output=True, timeout=5)
        except (subprocess.SubprocessError, FileNotFoundError):
            print(f"‚ùå Ollama n'est pas disponible. Veuillez installer et d√©marrer Ollama.")
            return result
        
        # V√©rifier si le mod√®le est disponible
        try:
            models_output = subprocess.run(["ollama", "list"], capture_output=True, text=True, timeout=5).stdout
            if self.model not in models_output:
                print(f"‚ùå Le mod√®le {self.model} n'est pas disponible. Veuillez l'installer avec 'ollama pull {self.model}'")
                return result
        except subprocess.SubprocessError:
            print(f"‚ö†Ô∏è Impossible de v√©rifier la disponibilit√© des mod√®les")
        
        # Cr√©er un fichier temporaire pour le prompt
        with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp:
            temp.write(prompt_text)
            temp_path = temp.name
        
        # Pr√©parer la commande Ollama avec pipe depuis le fichier
        cmd = f"cat {temp_path} | ollama run {self.model} 2>&1"
        
        # Initialiser le moniteur de ressources
        self.resource_monitor.start()
        
        response_text = ""
        start_time = time.time()
        
        try:
            # Ex√©cuter la commande avec timeout
            process = subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE, 
                shell=True, 
                text=True
            )
            
            # Lire la sortie en temps r√©el
            for _ in range(120):
                if process.poll() is not None:
                    break
                time.sleep(1)
            
            if process.poll() is None:
                process.terminate()
                process.wait(5)
                print(f"‚ö†Ô∏è Timeout apr√®s 120 secondes. Arr√™t forc√©.")
            
            # R√©cup√©rer toute la sortie
            stdout, stderr = process.communicate()
            response_text = stdout
            
            # Essayer d'extraire les informations de tokens du r√©sultat
            tokens_in = 0
            tokens_out = 0
            
            # Rechercher les informations de tokens dans la sortie d'erreur
            tokens_match = re.search(r'tokens: (\d+) in, (\d+) out', stderr, re.IGNORECASE)
            if tokens_match:
                tokens_in = int(tokens_match.group(1))
                tokens_out = int(tokens_match.group(2))
            else:
                # Utiliser une autre m√©thode si nous n'avons pas trouv√© les tokens
                # en estimant les tokens en fonction de la longueur du texte
                tokens_in = len(prompt_text) // 4  # Estimation grossi√®re: ~4 caract√®res par token
                
                # Si nous avons une r√©ponse, estimer les tokens de sortie
                if response_text and len(response_text) > 10:
                    tokens_out = len(response_text) // 4
        
        except subprocess.SubprocessError as e:
            print(f"‚ö†Ô∏è Erreur lors de l'ex√©cution: {e}")
        finally:
            # Supprimer le fichier temporaire
            try:
                os.unlink(temp_path)
            except:
                pass
        
        # Arr√™ter le monitoring et r√©cup√©rer les m√©triques
        self.resource_monitor.stop()
        execution_time = time.time() - start_time
        cpu_usage = self.resource_monitor.get_summary()["avg_cpu"]
        memory_usage = self.resource_monitor.get_summary()["avg_memory"]
        temperature = 0  # Temperature n'est pas disponible dans la sortie d'Ollama
        
        # V√©rifier si nous avons une r√©ponse valide
        if not response_text or len(response_text) < 20:
            print(f"‚ö†Ô∏è R√©ponse vide ou trop courte, tentative avec l'ex√©cution directe")
            
            # Essayer une autre approche - ex√©cution directe
            try:
                # √âchapper le prompt pour la ligne de commande
                escaped_prompt = prompt_text.replace('"', '\\"')
                direct_cmd = f'ollama run {self.model} "{escaped_prompt}" 2>&1'
                
                start_time = time.time()
                self.resource_monitor.start()
                
                response = subprocess.run(direct_cmd, shell=True, capture_output=True, text=True, timeout=10)
                response_text = response.stdout
                
                # Estimation des tokens bas√©e sur la longueur
                tokens_in = len(prompt_text) // 4
                tokens_out = len(response_text) // 4 if response_text else 0
                
                self.resource_monitor.stop()
                execution_time = time.time() - start_time
                cpu_usage = self.resource_monitor.get_summary()["avg_cpu"]
                memory_usage = self.resource_monitor.get_summary()["avg_memory"]
                temperature = 0  # Temperature n'est pas disponible dans la sortie directe
                
            except subprocess.SubprocessError as e:
                print(f"‚ö†Ô∏è Erreur lors de la seconde tentative: {e}")
        
        # Si nous avons un temps d'ex√©cution mais pas de tokens, faire une estimation
        if execution_time > 5 and (tokens_in == 0 or tokens_out == 0):
            print(f"‚ö†Ô∏è Aucun token d√©tect√© malgr√© l'ex√©cution. Estimation bas√©e sur la longueur...")
            tokens_in = max(tokens_in, len(prompt_text) // 4)
            tokens_out = max(tokens_out, (len(response_text) // 4) if response_text else 50)
        
        # Calculer l'√©nergie estim√©e (en Joules)
        # Formule r√©vis√©e: (CPU usage * Coefficient CPU + Memory usage * Coefficient Memory) * temps_execution / 100
        # Coefficients bas√©s sur des estimations r√©alistes pour un ordinateur standard
        # Diviser par 100 car CPU usage est en pourcentage
        coefficient_cpu = 2.5     # Watts par 100% de CPU 
        coefficient_memory = 0.05 # Watts par 100 MB de m√©moire
        
        # Calculer la consommation en Watts (puissance instantan√©e)
        power_watts = (cpu_usage * coefficient_cpu / 100) + (memory_usage * coefficient_memory / 1000)
        
        # Convertir Watts en Joules en multipliant par le temps d'ex√©cution
        energy_estimate = power_watts * execution_time
        
        # Plafonner l'√©nergie estim√©e √† une valeur raisonnable pour √©viter des r√©sultats aberrants
        energy_estimate = min(energy_estimate, 100)
        
        # Calculer le score √©cologique (inverse de l'√©nergie par token)
        # Plus le score est √©lev√©, meilleur c'est √©cologiquement
        tokens_total = tokens_in + tokens_out
        
        if energy_estimate > 0 and tokens_total > 0:
            tokens_per_joule = tokens_total / energy_estimate
            # Normaliser pour obtenir un score entre 0 et 100
            eco_score = min(100, tokens_per_joule * 5)
        else:
            tokens_per_joule = 0
            eco_score = 0
        
        # Si nous avons une ex√©cution mais un score √©cologique nul, donner une valeur minimale
        if execution_time > 5 and eco_score <= 0:
            eco_score = 1  # Valeur minimale pour permettre la comparaison
            tokens_per_joule = 0.2
        
        # Compl√©ter le r√©sultat
        result = PromptResult(
            prompt_id=prompt_id,
            prompt_text=prompt_text,
            prompt_length=len(prompt_text),
            response_text=response_text,
            response_length=len(response_text),
            tokens_in=tokens_in,
            tokens_out=tokens_out,
            execution_time=execution_time,
            avg_cpu_usage=cpu_usage,
            max_cpu_usage=self.resource_monitor.get_summary()["max_cpu"],
            avg_memory_mb=memory_usage,
            max_memory_mb=self.resource_monitor.get_summary()["max_memory"],
            avg_gpu_usage=self.resource_monitor.get_summary()["avg_gpu"],
            max_gpu_usage=self.resource_monitor.get_summary()["max_gpu"],
            energy_estimate_joules=energy_estimate,
            eco_score=eco_score
        )
        
        return result

def compare_prompts(prompts, model, repetitions=1, delay=5):
    """Compare plusieurs prompts et renvoie leurs r√©sultats"""
    results = []
    runner = PromptRunner(model)
    
    for i, (prompt_id, prompt_text) in enumerate(prompts.items()):
        print(f"\nAnalyse du prompt {i+1}/{len(prompts)}: {prompt_id}")
        
        # Ex√©cuter le prompt plusieurs fois pour obtenir une moyenne
        prompt_results = []
        for j in range(repetitions):
            print(f"  R√©p√©tition {j+1}/{repetitions}")
            result = runner.run_prompt(prompt_id, prompt_text)
            prompt_results.append(result)
            
            # Ajouter un d√©lai entre les ex√©cutions pour √©viter de surcharger Ollama
            if j < repetitions - 1:
                print(f"  Attente de {delay} secondes avant la prochaine r√©p√©tition...")
                time.sleep(delay)
        
        # Calculer les moyennes
        if not prompt_results:
            continue
            
        avg_result = PromptResult(
            prompt_id=prompt_id,
            prompt_text=prompt_text,
            prompt_length=prompt_results[0].prompt_length,
            response_text=prompt_results[0].response_text,
            response_length=prompt_results[0].response_length,
            tokens_in=statistics.mean([r.tokens_in for r in prompt_results]),
            tokens_out=statistics.mean([r.tokens_out for r in prompt_results]),
            execution_time=statistics.mean([r.execution_time for r in prompt_results]),
            avg_cpu_usage=statistics.mean([r.avg_cpu_usage for r in prompt_results]),
            max_cpu_usage=statistics.mean([r.max_cpu_usage for r in prompt_results]),
            avg_memory_mb=statistics.mean([r.avg_memory_mb for r in prompt_results]),
            max_memory_mb=statistics.mean([r.max_memory_mb for r in prompt_results]),
            avg_gpu_usage=statistics.mean([r.avg_gpu_usage for r in prompt_results]),
            max_gpu_usage=statistics.mean([r.max_gpu_usage for r in prompt_results]),
            energy_estimate_joules=statistics.mean([r.energy_estimate_joules for r in prompt_results]),
            eco_score=statistics.mean([r.eco_score for r in prompt_results])
        )
        
        # Ajouter des m√©triques d√©riv√©es
        avg_result_dict = avg_result.__dict__
        avg_result_dict["tokens_per_joule"] = avg_result.tokens_out / max(0.01, avg_result.energy_estimate_joules)
        avg_result_dict["energy_per_token"] = avg_result.energy_estimate_joules / max(1, avg_result.tokens_out)
        avg_result_dict["tokens_per_second"] = avg_result.tokens_out / max(0.01, avg_result.execution_time)
        
        # Stocker le r√©sultat
        results.append(avg_result_dict)
        
        # Ajouter un d√©lai entre les prompts
        if i < len(prompts) - 1:
            print(f"\nPause de {delay} secondes avant le prochain prompt...")
            time.sleep(delay)
    
    return results

def generate_comparison_visualizations(results, output_dir="vizs"):
    """
    G√©n√®re des visualisations comparant les diff√©rents prompts
    """
    if not results:
        print("Aucun r√©sultat √† visualiser.")
        return
    
    # Cr√©er le r√©pertoire de sortie s'il n'existe pas
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # Convertir en DataFrame pour faciliter la visualisation
    # Les r√©sultats peuvent √™tre soit des objets PromptResult, soit des dictionnaires
    if isinstance(results[0], dict):
        df = pd.DataFrame(results)
    else:
        df = pd.DataFrame([r.__dict__ for r in results])
    
    # V√©rifier et corriger les valeurs NaN ou z√©ro pour √©viter des probl√®mes dans les visualisations
    for col in ["eco_score", "execution_time", "energy_estimate_joules", "tokens_out", "tokens_per_joule"]:
        if col in df.columns:
            # Remplacer les NaN et les z√©ros par une petite valeur positive
            df[col] = df[col].replace({0: 0.01, np.nan: 0.01})
    
    # Simuler un score √©cologique bas√© sur la longueur du prompt si aucun score valide n'est d√©tect√©
    simulated_scores = False
    if df["eco_score"].mean() <= 0.5:  # Si les scores sont tr√®s bas, ils sont probablement invalides
        print("Simulation des scores √©cologiques bas√©e sur la longueur des prompts...")
        df["eco_score"] = 100 - (df["prompt_length"] / df["prompt_length"].max() * 90)
        simulated_scores = True
    
    # Configurer un style plus simple et color√©
    plt.style.use('seaborn-v0_8-pastel')
    
    # Cr√©er des versions courtes des textes de prompts pour l'affichage
    df['prompt_short'] = df['prompt_text'].apply(lambda x: (x[:25] + '...') if len(x) > 25 else x)
    
    # 1. GRAPHIQUE PRINCIPAL: Score √©cologique avec ic√¥nes et √©chelle visuelle
    plt.figure(figsize=(12, 8))
    
    # Trier par score √©cologique
    df_sorted = df.sort_values("eco_score", ascending=False)
    
    # Palette de couleurs du rouge au vert
    bars = plt.bar(
        df_sorted['prompt_short'], 
        df_sorted['eco_score'],
        color=[(0, min(x/100, 0.8), 0) for x in df_sorted['eco_score']],
        width=0.6
    )
    
    # Ajouter des labels de valeur sur chaque barre
    for bar in bars:
        height = bar.get_height()
        rating_text = ""
        if height >= 80:
            rating_text = "üåü Excellent"
        elif height >= 60:
            rating_text = "‚ú® Tr√®s bon"
        elif height >= 40:
            rating_text = "üëç Bon"
        elif height >= 20:
            rating_text = "üëå Acceptable"
        else:
            rating_text = "‚ö†Ô∏è Faible"
            
        plt.text(
            bar.get_x() + bar.get_width()/2.,
            height + 5,
            f"{int(height)}/100\n{rating_text}",
            ha='center',
            va='bottom',
            fontweight='bold',
            color='#333333'
        )
    
    plt.title("üåø Score √âcologique des Prompts" + (" (estimation)" if simulated_scores else ""), fontsize=16, fontweight='bold')
    plt.xlabel("Formulation du prompt", fontsize=12)
    plt.ylabel("Score (0-100)", fontsize=12)
    
    # Ajouter des zones color√©es pour une √©chelle de lecture rapide
    plt.axhspan(0, 20, alpha=0.1, color='red', label='Faible')
    plt.axhspan(20, 40, alpha=0.1, color='orange', label='Acceptable')
    plt.axhspan(40, 60, alpha=0.1, color='yellow', label='Bon')
    plt.axhspan(60, 80, alpha=0.1, color='lightgreen', label='Tr√®s bon')
    plt.axhspan(80, 100, alpha=0.1, color='green', label='Excellent')
    
    plt.ylim(0, 110)  # Donner de l'espace pour les labels
    plt.legend(title="√âchelle de performance", bbox_to_anchor=(1, 0.5), loc='center left')
    plt.xticks(rotation=30, ha='right')
    plt.tight_layout()
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.savefig(f"{output_dir}/score_ecologique.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. TEMPS DE R√âPONSE avec visualisation intuitive
    plt.figure(figsize=(12, 6))
    
    # Trier par temps d'ex√©cution
    df_sorted = df.sort_values("execution_time")
    
    # Utiliser des couleurs pour indiquer la rapidit√© (vert=rapide, rouge=lent)
    colors = ['#4CAF50' if t < 10 else '#FFC107' if t < 30 else '#F44336' for t in df_sorted["execution_time"]]
    
    # Cr√©er le graphique √† barres
    bars = plt.bar(df_sorted['prompt_short'], df_sorted['execution_time'], color=colors, width=0.6)
    
    # Ajouter une ic√¥ne et une cat√©gorie pour chaque barre
    for bar in bars:
        height = bar.get_height()
        speed_icon = "‚ö°" if height < 10 else "üïí" if height < 30 else "‚åõ"
        speed_text = "Tr√®s rapide" if height < 10 else "Normal" if height < 30 else "Lent"
        
        plt.text(
            bar.get_x() + bar.get_width()/2.,
            height + 1,
            f"{speed_icon} {speed_text}\n{height:.1f}s",
            ha='center',
            va='bottom',
            fontweight='bold'
        )
    
    plt.title("‚è±Ô∏è Temps de R√©ponse", fontsize=16, fontweight='bold')
    plt.xlabel("Formulation du prompt", fontsize=12)
    plt.ylabel("Temps (secondes)", fontsize=12)
    
    # Ajouter des zones pour identifier les tranches de temps
    rapide_max = 10
    normal_max = 30
    
    plt.axhspan(0, rapide_max, alpha=0.1, color='green', label='Tr√®s rapide (0-10s)')
    plt.axhspan(rapide_max, normal_max, alpha=0.1, color='orange', label='Normal (10-30s)')
    plt.axhspan(normal_max, df_sorted['execution_time'].max() * 1.2, alpha=0.1, color='red', label='Lent (>30s)')
    
    plt.legend(title="Cat√©gories de vitesse", bbox_to_anchor=(1, 0.5), loc='center left')
    plt.xticks(rotation=30, ha='right')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/temps_reponse.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. GRAPHIQUE √âNERGIE avec analogies visuelles
    plt.figure(figsize=(12, 6))
    
    # Trier par consommation √©nerg√©tique (plus petit = meilleur)
    df_sorted = df.sort_values("energy_estimate_joules")
    
    # Cr√©er le graphique √† barres
    bars = plt.bar(
        df_sorted['prompt_short'], 
        df_sorted['energy_estimate_joules'],
        color=['#8BC34A', '#CDDC39', '#FFEB3B'][:len(df_sorted)],
        width=0.6
    )
    
    # Ajouter des analogies pour chaque barre
    for bar in bars:
        height = bar.get_height()
        
        # Cr√©er une analogie visuelle
        if height < 1:
            icon = "üí°"
            analogy = "< 1 seconde de LED"
        elif height < 5:
            icon = "üí°"
            analogy = "= quelques sec. de LED"
        elif height < 20:
            icon = "üí°üí°"
            analogy = "= 20 sec. de LED"
        else:
            icon = "üí°üí°üí°"
            analogy = f"= {int(height)} sec. de LED"
            
        plt.text(
            bar.get_x() + bar.get_width()/2.,
            height + (df_sorted['energy_estimate_joules'].max() * 0.05),
            f"{height:.2f}J\n{icon} {analogy}",
            ha='center',
            va='bottom',
            fontweight='bold'
        )
    
    plt.title("‚ö° Consommation √ânerg√©tique", fontsize=16, fontweight='bold')
    plt.xlabel("Formulation du prompt", fontsize=12)
    plt.ylabel("√ânergie (Joules)", fontsize=12)
    plt.xticks(rotation=30, ha='right')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/energie_consommee.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 4. GRAPHIQUE COMPARATIF GLOBAL - Diagramme en radar simplifi√©
    # Limiter √† max 4 prompts pour la lisibilit√©
    num_prompts_radar = min(len(df), 4)
    
    if num_prompts_radar >= 2:  # Besoin d'au moins 2 prompts pour un radar
        plt.figure(figsize=(10, 10))
        
        # S√©lectionner les premi√®res lignes pour le radar (max 4)
        df_radar = df.head(num_prompts_radar).copy()
        
        # S√©lectionner des m√©triques faciles √† comprendre
        metrics = [
            ("Score √©cologique", "eco_score", True),  # Nom, colonne, Plus grand = meilleur?
            ("Rapidit√©", "execution_time", False),
            ("√âconomie d'√©nergie", "energy_estimate_joules", False),
            ("Concision", "prompt_length", False),
            ("Tokens g√©n√©r√©s", "tokens_out", True)
        ]
        
        # Normaliser les m√©triques pour le radar (0-1 o√π 1 est toujours meilleur)
        for name, col, higher_is_better in metrics:
            if col in df_radar.columns:
                if higher_is_better:
                    max_val = max(df_radar[col].max(), 0.001)
                    df_radar[f"{col}_norm"] = df_radar[col] / max_val
                else:
                    # Pour les m√©triques o√π plus petit est meilleur (inverser)
                    min_val = df_radar[col].min()
                    max_val = df_radar[col].max()
                    if max_val > min_val:
                        df_radar[f"{col}_norm"] = 1 - ((df_radar[col] - min_val) / (max_val - min_val))
                    else:
                        df_radar[f"{col}_norm"] = 1.0  # Toutes les valeurs sont √©gales
            else:
                # Si la m√©trique n'existe pas, utiliser une valeur par d√©faut
                df_radar[f"{col}_norm"] = 0.5
        
        # Pr√©parer les cat√©gories et angles pour le radar
        categories = [name for name, _, _ in metrics]
        N = len(categories)
        
        # Angles pour le radar chart
        angles = [n / float(N) * 2 * np.pi for n in range(N)]
        angles += angles[:1]  # Fermer le cercle
        
        # Couleurs vives pour chaque prompt
        colors = ['#FF5722', '#2196F3', '#4CAF50', '#9C27B0']
        
        # Initialiser le radar chart
        ax = plt.subplot(111, polar=True)
        
        # Dessiner chaque prompt
        for i in range(num_prompts_radar):
            values = [df_radar.iloc[i][f"{col}_norm"] for _, col, _ in metrics]
            values += values[:1]  # Fermer le polygone
            
            # Dessiner la ligne
            ax.plot(angles, values, linewidth=2.5, label=df_radar.iloc[i]['prompt_short'], color=colors[i])
            # Remplir la zone
            ax.fill(angles, values, alpha=0.25, color=colors[i])
        
        # Dessiner les axes et labels
        plt.xticks(angles[:-1], categories, size=14, fontweight='bold')
        
        # Ajouter des cercles de r√©f√©rence
        ax.set_rlabel_position(0)
        plt.yticks([0.25, 0.5, 0.75], ["Faible", "Moyen", "√âlev√©"], color="grey", size=10)
        plt.ylim(0, 1)
        
        # Ajouter la l√©gende avec des couleurs distinctes
        plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), frameon=True, ncol=2, fontsize=12)
        
        plt.title("üìä Comparaison Globale des Performances", fontsize=16, fontweight='bold', y=1.1)
        plt.tight_layout()
        plt.savefig(f"{output_dir}/comparaison_radar.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    # 5. Infographie r√©capitulative montrant le gagnant
    plt.figure(figsize=(12, 8))
    
    # Trier par score √©cologique
    best_prompt = df.sort_values("eco_score", ascending=False).iloc[0]
    
    # Cr√©er un visuel simple avec le gagnant et ses avantages
    plt.text(0.5, 0.9, "üèÜ LE PROMPT LE PLUS √âCOLOGIQUE üèÜ", 
             fontsize=20, fontweight='bold', ha='center', va='center')
    
    plt.text(0.5, 0.8, f"\"{best_prompt['prompt_text']}\"", 
             fontsize=16, ha='center', va='center', 
             bbox=dict(facecolor='#E8F5E9', alpha=0.8, boxstyle='round,pad=1'))
    
    # Cr√©er des ic√¥nes et m√©triques
    metrics_text = ""
    metrics_text += f"üåø Score √©cologique: {best_prompt['eco_score']:.1f}/100\n\n"
    metrics_text += f"‚è±Ô∏è Temps de r√©ponse: {best_prompt['execution_time']:.1f} secondes\n\n"
    metrics_text += f"‚ö° √ânergie consomm√©e: {best_prompt['energy_estimate_joules']:.2f} joules\n\n"
    metrics_text += f"üìè Longueur: {best_prompt['prompt_length']} caract√®res\n\n"
    metrics_text += f"üí¨ Tokens g√©n√©r√©s: {int(best_prompt['tokens_out'])}"
    
    plt.text(0.5, 0.5, metrics_text, 
             fontsize=14, ha='center', va='center', linespacing=1.8,
             bbox=dict(facecolor='#F1F8E9', alpha=0.8, boxstyle='round,pad=1'))
    
    # Ajouter un titre explicatif
    plt.text(0.5, 0.15, "‚úÖ CONSEIL: Utilisez des prompts courts et pr√©cis\npour r√©duire l'impact environnemental", 
             fontsize=16, fontweight='bold', ha='center', va='center', color='#2E7D32')
    
    # Enlever les axes
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/prompt_gagnant.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"‚úÖ Visualisations simplifi√©es g√©n√©r√©es dans le r√©pertoire {output_dir}/")
    print(f"  - score_ecologique.png: Comparaison des scores √©cologiques")
    print(f"  - temps_reponse.png: Comparaison des temps de r√©ponse")
    print(f"  - energie_consommee.png: Comparaison des consommations √©nerg√©tiques")
    if num_prompts_radar >= 2:
        print(f"  - comparaison_radar.png: Vue d'ensemble comparative")
    print(f"  - prompt_gagnant.png: Infographie du prompt gagnant")

def generate_comparison_report(results, output_file="rapport_comparaison_prompts.md"):
    """G√©n√®re un rapport comparatif entre les prompts test√©s, accessible aux non-techniciens"""
    if not results or len(results) == 0:
        print("Aucun r√©sultat pour g√©n√©rer un rapport")
        return
        
    # Trier les r√©sultats par score √©cologique
    sorted_results = sorted(results, key=lambda x: x["eco_score"], reverse=True)
    
    # Ajouter un classement en pourcentage relatif
    best_score = sorted_results[0]["eco_score"]
    for result in sorted_results:
        if best_score > 0:
            result["score_relatif"] = (result["eco_score"] / best_score) * 100
        else:
            result["score_relatif"] = 0
    
    # G√©n√©rer le rapport
    report = f"""# üåø Rapport de comparaison des prompts √©cologiques

## R√©sum√© en langage simple

Ce rapport compare **{len(results)} formulations diff√©rentes** d'une m√™me question pos√©e √† l'IA.

Les r√©sultats montrent **quel prompt consomme le moins d'√©nergie** tout en obtenant une r√©ponse satisfaisante.

### üèÜ Classement des prompts

"""
    
    # Tableau des r√©sultats simplifi√©
    report += "| Classement | Formulation | Eco-score | Efficacit√© relative | Temps de r√©ponse |\n"
    report += "|------------|-------------|-----------|---------------------|------------------|\n"
    
    for rank, result in enumerate(sorted_results, 1):
        eco_score = result["eco_score"]
        # Convertir le score relatif en ic√¥nes
        if result["score_relatif"] > 90:
            eco_rating = "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Excellent)"
        elif result["score_relatif"] > 75:
            eco_rating = "‚≠ê‚≠ê‚≠ê‚≠ê (Tr√®s bon)"
        elif result["score_relatif"] > 50:
            eco_rating = "‚≠ê‚≠ê‚≠ê (Bon)"
        elif result["score_relatif"] > 25:
            eco_rating = "‚≠ê‚≠ê (Moyen)"
        else:
            eco_rating = "‚≠ê (Faible)"
            
        # Convertir le temps en texte
        if result["execution_time"] < 10:
            time_rating = "‚ö° Tr√®s rapide"
        elif result["execution_time"] < 30:
            time_rating = "‚ö° Rapide"
        elif result["execution_time"] < 60:
            time_rating = "üïí Moyen"
        else:
            time_rating = "‚è±Ô∏è Long"
        
        # Formater le prompt pour le tableau (version courte)
        prompt_text = result["prompt_text"]
        short_prompt = prompt_text[:40] + "..." if len(prompt_text) > 40 else prompt_text
        
        report += f"| {rank} | {short_prompt} | {eco_score:.1f} | {eco_rating} | {time_rating} |\n"
    
    report += f"""
## üí° Ce que ces r√©sultats signifient

L'**eco-score** est un nombre qui indique l'efficacit√© √©cologique du prompt. **Plus le score est √©lev√©, mieux c'est!**

L'**efficacit√© relative** compare tous les prompts avec le meilleur d'entre eux.

Le **temps de r√©ponse** indique la rapidit√© avec laquelle l'IA a r√©pondu.

## üîç D√©tails des prompts test√©s

"""
    
    for result in sorted_results:
        report += f"### Prompt {sorted_results.index(result) + 1}: \"{result['prompt_text']}\"\n\n"
        
        # Cr√©er un r√©sum√© en langage simple
        length_desc = "court" if result['prompt_length'] < 50 else "moyen" if result['prompt_length'] < 150 else "long"
        report += f"**Nombre de caract√®res**: {result['prompt_length']} ({length_desc})\n\n"
        
        # Cr√©er une repr√©sentation visuelle de l'efficacit√©
        eco_bar = "üü©" * int(result["score_relatif"] / 10)
        eco_bar += "‚¨ú" * (10 - int(result["score_relatif"] / 10))
        
        energy_value = result["energy_estimate_joules"]
        # Analogie pour la consommation d'√©nergie
        if energy_value < 1:
            energy_analogy = "moins qu'une LED pendant 1 seconde"
        elif energy_value < 5:
            energy_analogy = "√©quivalent √† une LED pendant quelques secondes"
        elif energy_value < 20:
            energy_analogy = "comme une ampoule LED pendant 20 secondes"
        else:
            energy_analogy = f"comparable √† une ampoule LED pendant {int(energy_value)} secondes"
        
        report += f"""**Performances √©cologiques**:
- Eco-score: {result['eco_score']:.1f} points
- Efficacit√©: {result['score_relatif']:.0f}% du meilleur prompt {eco_bar}
- √ânergie consomm√©e: {result['energy_estimate_joules']:.2f} joules ({energy_analogy})
- Temps de r√©ponse: {result['execution_time']:.1f} secondes
- Nombre de mots g√©n√©r√©s: environ {int(result['tokens_out'] * 0.75)} mots
\n\n"""
        
        # Ajouter un extrait de la r√©ponse
        response_extract = result['response_text'][:200] + "..." if len(result['response_text']) > 200 else result['response_text']
        report += f"**Extrait de la r√©ponse**:\n```\n{response_extract}\n```\n\n"
    
    report += """## üå± Conseils pour des prompts plus √©cologiques

Selon cette analyse, les prompts les plus √©cologiques sont ceux qui:

1. **Sont concis** - Utilisez moins de mots pour poser votre question
2. **Sont pr√©cis** - Allez droit au but sans phrases polies excessives
3. **Sont bien structur√©s** - Une organisation claire aide l'IA √† r√©pondre plus efficacement

En appliquant ces principes simples, vous pouvez r√©duire l'empreinte √©cologique de vos interactions avec l'IA de **25% √† 75%** !

## üîß D√©tails techniques

Pour les lecteurs int√©ress√©s par les aspects techniques, voici quelques m√©triques suppl√©mentaires:

"""
    
    # Tableau technique complet pour ceux qui veulent les d√©tails
    report += "| Prompt | Tokens entr√©e | Tokens sortie | CPU moyen | M√©moire (MB) | Tokens/Joule | Tokens/Seconde |\n"
    report += "|--------|---------------|---------------|-----------|--------------|--------------|----------------|\n"
    
    for result in sorted_results:
        id = sorted_results.index(result) + 1
        report += f"| {id} | {result['tokens_in']:.0f} | {result['tokens_out']:.0f} | {result['avg_cpu_usage']:.1f}% | {result['avg_memory_mb']:.0f} | {result['tokens_per_joule']:.1f} | {result['tokens_per_second']:.1f} |\n"
    
    # Enregistrer le rapport
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
        
    print(f"Rapport de comparaison simplifi√© g√©n√©r√©: {output_file}")

def main():
    parser = argparse.ArgumentParser(
        description="Comparaison √©cologique de prompts pour Ollama",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
-----------------------
1. Comparer deux prompts simples:
   python scripts/compare_prompts.py --prompt1 "Explique l'effet photo√©lectrique" --prompt2 "Effet photo√©lectrique?"

2. Comparer trois formulations diff√©rentes:
   python scripts/compare_prompts.py \\
      --prompt1 "Explique l'effet photo√©lectrique" \\
      --prompt2 "Effet photo√©lectrique?" \\
      --prompt3 "Explique l'effet photo√©lectrique en 3 points"

3. Utiliser un mod√®le sp√©cifique:
   python scripts/compare_prompts.py --model llama2 --prompt1 "..." --prompt2 "..."
   
4. Mode rapide (pour tests):
   python scripts/compare_prompts.py --mode fast --prompt1 "..." --prompt2 "..."
"""
    )
    
    parser.add_argument("--model", default=DEFAULT_MODEL, help=f"Mod√®le Ollama √† utiliser (d√©faut: {DEFAULT_MODEL})")
    parser.add_argument("--repetitions", type=int, default=DEFAULT_REPETITIONS, 
                        help=f"Nombre de r√©p√©titions pour chaque prompt (d√©faut: {DEFAULT_REPETITIONS})")
    parser.add_argument("--delay", type=int, default=5,
                        help="D√©lai en secondes entre l'analyse de chaque prompt (d√©faut: 5)")
    parser.add_argument("--output-dir", default="resultats_comparaison",
                        help="Dossier pour les visualisations (d√©faut: resultats_comparaison)")
    parser.add_argument("--prompt1", required=True, help="Premier prompt √† comparer")
    parser.add_argument("--prompt2", required=True, help="Deuxi√®me prompt √† comparer")
    parser.add_argument("--prompt3", help="Troisi√®me prompt √† comparer (optionnel)")
    parser.add_argument("--mode", choices=["normal", "fast"], default="normal",
                        help="Mode d'ex√©cution: normal ou fast (rapide, pour tests)")
    
    args = parser.parse_args()
    
    # Message d'accueil
    print("\n" + "=" * 80)
    print(f"{'COMPARAISON √âCOLOGIQUE DE PROMPTS':^80}")
    print("=" * 80)
    print(f"Ce script permet de comparer des formulations de prompts pour ")
    print(f"d√©terminer laquelle est la plus efficace √©cologiquement.")
    print(f"Mod√®le utilis√©: \033[1m{args.model}\033[0m")
    print(f"Nombre de prompts √† comparer: \033[1m{2 + (1 if args.prompt3 else 0)}\033[0m")
    print(f"Mode: \033[1m{args.mode}\033[0m")
    print("=" * 80 + "\n")
    
    # Afficher les prompts qui seront compar√©s
    print("Prompts √† comparer:")
    print(f"1. \"{args.prompt1[:60]}{'...' if len(args.prompt1) > 60 else ''}\"")
    print(f"2. \"{args.prompt2[:60]}{'...' if len(args.prompt2) > 60 else ''}\"")
    if args.prompt3:
        print(f"3. \"{args.prompt3[:60]}{'...' if len(args.prompt3) > 60 else ''}\"")
    print()
    
    # V√©rifier que Ollama est disponible
    print("‚è≥ V√©rification de la disponibilit√© d'Ollama...")
    try:
        subprocess.run(["ollama", "list"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        print("‚úÖ Ollama est correctement install√© et disponible")
    except subprocess.CalledProcessError:
        print("‚ùå Erreur: Ollama n'est pas correctement install√© ou n'est pas disponible")
        return 1
    except FileNotFoundError:
        print("‚ùå Erreur: Ollama n'est pas install√© ou n'est pas dans le PATH")
        return 1
        
    # Cr√©er le r√©pertoire de sortie
    os.makedirs(args.output_dir, exist_ok=True)
    print(f"üìÅ Les r√©sultats seront enregistr√©s dans: {args.output_dir}")
    
    # Avertissement sur le temps n√©cessaire
    prompt_count = 2 + (1 if args.prompt3 else 0)
    
    # Ajuster les param√®tres si mode rapide
    if args.mode == "fast":
        print("\nüöÄ MODE RAPIDE: Tests acc√©l√©r√©s avec temps d'ex√©cution r√©duit")
        estimated_time = prompt_count * 30  # 30 secondes par prompt en mode rapide
        # On va simuler une partie des r√©sultats pour acc√©l√©rer les tests
        
        # Pr√©parer les prompts √† comparer
        prompts = {}
        prompts["Prompt 1"] = args.prompt1
        prompts["Prompt 2"] = args.prompt2
        if args.prompt3:
            prompts["Prompt 3"] = args.prompt3
            
        # Simuler les r√©sultats en mode rapide
        results = []
        for prompt_id, prompt_text in prompts.items():
            # Cr√©er un r√©sultat simul√© mais r√©aliste bas√© sur la longueur du prompt
            prompt_length = len(prompt_text)
            execution_time = max(0.5, prompt_length / 30)  # Plus long prompt = plus long temps d'ex√©cution
            energy = max(0.1, prompt_length / 200)  # Estimation de l'√©nergie bas√©e sur la longueur
            
            # Plus court = meilleur score √©cologique (simulation)
            eco_score = 100 - (prompt_length / (max([len(p) for p in prompts.values()]) * 0.9))
            
            # Estimer les tokens selon la longueur
            tokens_in = max(1, prompt_length // 4)
            tokens_out = max(10, prompt_length // 2)  # Simulation de r√©ponse plus longue que la question
            
            result = {
                "prompt_id": prompt_id,
                "prompt_text": prompt_text,
                "prompt_length": prompt_length,
                "response_text": f"Ceci est une r√©ponse simul√©e pour {prompt_text[:20]}...",
                "response_length": tokens_out * 4,
                "tokens_in": tokens_in,
                "tokens_out": tokens_out,
                "execution_time": execution_time,
                "avg_cpu_usage": 20 + (prompt_length / 10),
                "max_cpu_usage": 30 + (prompt_length / 8),
                "avg_memory_mb": 500 + (prompt_length * 2),
                "max_memory_mb": 600 + (prompt_length * 2.5),
                "energy_estimate_joules": energy,
                "eco_score": eco_score,
                "tokens_per_joule": tokens_out / max(0.1, energy),
                "energy_per_token": energy / max(1, tokens_out),
                "tokens_per_second": tokens_out / max(0.1, execution_time)
            }
            results.append(result)
            print(f"‚úì Simulation du prompt {prompt_id}")
    else:
        # Mode normal avec ex√©cution compl√®te
        estimated_time = prompt_count * 2 * 60  # 2 minutes par prompt en moyenne
        
        # Pr√©parer les prompts √† comparer
        prompts = {}
        prompts["Prompt 1"] = args.prompt1
        prompts["Prompt 2"] = args.prompt2
        if args.prompt3:
            prompts["Prompt 3"] = args.prompt3
            
        print("\nüöÄ D√©marrage de l'analyse comparative...")
        
        # Comparer les prompts
        start_time = time.time()
        results = compare_prompts(
            prompts=prompts,
            model=args.model,
            repetitions=args.repetitions,
            delay=args.delay
        )
        total_time = time.time() - start_time
    
    minutes = estimated_time // 60
    seconds = estimated_time % 60
    
    if args.mode == "normal":
        print(f"\n‚ö†Ô∏è  Cette analyse peut prendre jusqu'√† {minutes} minutes et {seconds} secondes.")
        print("   Chaque prompt sera envoy√© √† Ollama avec un timeout de 120 secondes.")
        
        proceed = input("\nAppuyez sur Entr√©e pour continuer ou Ctrl+C pour annuler... ")
    
    if not results:
        print("‚ùå Aucun r√©sultat n'a √©t√© obtenu")
        return 1
        
    # G√©n√©rer les visualisations
    print("\nüìä G√©n√©ration des visualisations...")
    generate_comparison_visualizations(results, args.output_dir)
    
    # G√©n√©rer le rapport
    print("\nüìù G√©n√©ration du rapport...")
    report_path = os.path.join(args.output_dir, "rapport_comparaison.md")
    generate_comparison_report(results, report_path)
    
    # R√©sum√© des r√©sultats
    print("\n" + "=" * 80)
    print(f"{'R√âSULTATS DE LA COMPARAISON':^80}")
    print("=" * 80)
    
    # Trier les r√©sultats par score √©cologique
    sorted_results = sorted(results, key=lambda x: x["eco_score"], reverse=True)
    
    for rank, result in enumerate(sorted_results, 1):
        prompt_text = result["prompt_text"]
        short_prompt = prompt_text[:40] + "..." if len(prompt_text) > 40 else prompt_text
        eco_score = result["eco_score"]
        energy = result["energy_estimate_joules"]
        time_taken = result["execution_time"]
        
        # Afficher le classement avec emoji selon le rang
        rank_emoji = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else f"{rank}."
        print(f"{rank_emoji} \"{short_prompt}\"")
        print(f"   Score √©cologique: {eco_score:.1f} | √ânergie: {energy:.2f}J | Temps: {time_taken:.1f}s")
        
        # Donner un exemple concret de consommation
        energy_examples = get_energy_examples(energy)
        print(f"   üîã √âquivalence: {energy_examples}")
    
    print("\n" + "=" * 80)
    if args.mode == "normal":
        minutes = int(total_time // 60)
        seconds = int(total_time % 60)
        print(f"Analyse termin√©e en {minutes}m {seconds}s!")
    else:
        print(f"Simulation en mode rapide termin√©e!")
    print(f"üìÇ R√©sultats disponibles dans: {os.path.abspath(args.output_dir)}")
    print(f"üìä Graphiques g√©n√©r√©s: score_ecologique.png, temps_reponse.png, energie_consommee.png")
    print(f"üìÑ Rapport complet: {os.path.abspath(report_path)}")
    print("=" * 80)
    
    return 0

def get_energy_examples(joules):
    """Convertit l'√©nergie en exemples concrets et compr√©hensibles"""
    if joules < 0.01:
        return "Infime (moins que l'√©nergie pour d√©placer une feuille de papier)"
    elif joules < 0.1:
        return f"Comme l'√©nergie pour lever une pi√®ce de monnaie de 1 cm"
    elif joules < 0.5:
        return f"‚âà L'√©nergie pour tourner une page de livre ({joules*100:.0f} centi√®mes de joule)"
    elif joules < 1:
        return f"‚âà {joules:.2f}J (allumer une LED pendant {joules*10:.1f} secondes)"
    elif joules < 5:
        return f"‚âà L'√©nergie pour soulever un smartphone de 10 cm"
    elif joules < 10:
        return f"‚âà Chauffer 2ml de caf√© de {joules/8.4:.1f}¬∞C"
    elif joules < 20:
        return f"‚âà La chaleur d√©gag√©e par votre doigt en 1 minute"
    elif joules < 50:
        return f"‚âà L'√©nergie pour faire fonctionner un smartphone pendant {joules/15:.1f} secondes"
    elif joules < 100:
        return f"‚âà Une goutte d'essence (‚âà {joules/33:.2f}mL)"
    else:
        return f"‚âà Faire fonctionner un ordinateur portable pendant {joules/40:.1f} secondes"

if __name__ == "__main__":
    sys.exit(main()) 